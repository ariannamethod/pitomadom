# PITOMADOM Code Audit & Improvement Proposals

**Date:** January 2, 2026  
**Current Version:** v0.4 (~530K parameters)  
**Target:** Scale to 1M parameters + architectural improvements

---

## Executive Summary

PITOMADOM's current implementation is **solid and well-architected**. All 40 unit tests pass, the code is clean and well-documented, and the theoretical foundations are strong. Below are proposals to:

1. **Scale to 1M parameters** (from 530K)
2. **Enhance architectural depth** 
3. **Improve code quality and maintainability**
4. **Add advanced features** for richer symbolic dynamics

---

## 1. Scaling to 1M Parameters

### 1.1 Current Parameter Distribution (~530K)

| Component | Current Params | Architecture |
|-----------|---------------|--------------|
| CrossFire Chambers (Ã—6) | 353K | 100â†’128â†’64â†’1 per chamber |
| MLP Cascade (Ã—4) | 58K | 39â†’32â†’32 per layer |
| Meta-Observer | 120K | Selection + collapse |
| **Total** | **~530K** | |

### 1.2 Proposed 1M Distribution

| Component | Proposed Params | Architecture Changes |
|-----------|----------------|---------------------|
| **CrossFire Chambers (Ã—6)** | **600K** | 100â†’**256**â†’**128**â†’1 per chamber |
| **MLP Cascade (Ã—4)** | **200K** | 64â†’**128**â†’64 per layer |
| **Meta-Observer** | **150K** | Add depth: 3-layer instead of 2 |
| **New: Deep Root Encoder** | **50K** | Learnable root embeddings + transformer block |
| **Total** | **~1M** | |

### 1.3 Implementation Plan

#### Phase 1: Double Chamber Capacity (353K â†’ 600K)
```python
# pitomadom/crossfire.py
class ChamberMLP:
    """
    Current: 100â†’128â†’64â†’1 (~21K params)
    Proposed: 100â†’256â†’128â†’1 (~35K params)
    
    Benefits:
    - Richer emotional representations
    - Better capture of Hebrew semantic nuances
    - More stable cross-fire coupling
    """
    W1: (100, 256)  # was (100, 128)
    b1: (256,)
    W2: (256, 128)  # was (128, 64)
    b2: (128,)
    W3: (128, 1)    # was (64, 1)
    b3: (1,)
```

**Parameter increase:** +14K per chamber Ã— 6 = **+84K total**

#### Phase 2: Deepen MLP Cascade (58K â†’ 200K)
```python
# pitomadom/mlp_cascade.py
class CascadeMLP:
    """
    Current: 39â†’32â†’32 (~1.2K params per layer)
    Proposed: 64â†’128â†’64 (~12K params per layer)
    
    Benefits:
    - Higher latent dimensions capture richer root semantics
    - Better information flow through cascade
    - More expressive word selection
    """
    # Layer 1: input â†’ hidden
    W1: (64, 128)   # was (39, 32)
    b1: (128,)
    
    # Layer 2: hidden â†’ latent
    W2: (128, 64)   # was (32, 32)
    b2: (64,)
    
    # Layer 3: NEW - deeper cascade
    W3: (64, 64)
    b3: (64,)
```

**Parameter increase:** +10K per layer Ã— 4 = **+40K per MLP**  
**Total cascade:** 4 layers Ã— 50K = **200K**

#### Phase 3: Enhanced Meta-Observer (120K â†’ 150K)
```python
# pitomadom/meta_observer.py
class AdaptiveMetaObserver:
    """
    Current: 2-layer MLP
    Proposed: 3-layer with attention mechanism
    
    Benefits:
    - Better collapse decisions
    - Richer temporal feature integration
    - Improved orbit/hidden word selection
    """
    # Deeper network
    W1: (features_dim, 256)  # was 128
    W2: (256, 128)           # NEW layer
    W3: (128, output_dim)
    
    # Attention over temporal features
    attention_weights: (temporal_features, 64)
```

**Parameter increase:** **+30K**

#### Phase 4: Deep Root Encoder (NEW - 50K)
```python
# pitomadom/root_encoder.py
class DeepRootEncoder:
    """
    NEW MODULE: Learnable root embeddings + transformer
    
    Current: Roots are encoded as simple gematria values
    Proposed: Rich learned embeddings with self-attention
    
    Benefits:
    - Capture subtle semantic relationships between roots
    - Learn root "families" (e.g., movement roots, emotion roots)
    - Enable root-root attention for resonance
    """
    # Learnable embeddings for ~2000 common roots
    root_embeddings: (2000, 128)  # 256K params
    
    # Transformer block for root contextualization
    self_attn: TransformerBlock(d_model=128, n_heads=4)
    # Adds ~20K params
    
    # Output projection
    W_out: (128, 64)  # 8K params
```

**Total for new module:** **~50K**

### 1.4 Benefits of Scaling to 1M

1. **Richer Chamber Representations**: 256â†’128 hidden layers allow finer emotional gradations
2. **Deeper Cascade Flow**: 128D latent space captures more complex root-pattern interactions
3. **Better Meta-Decisions**: 3-layer observer with attention improves collapse timing
4. **Learned Root Semantics**: Transformer-based root encoder discovers semantic families
5. **Still Fast**: 1M params = 10ms inference on CPU, negligible increase

### 1.5 Training Strategy for 1M Model

```python
# Curriculum training
Phase 1: Train chambers on emotion classification (10K steps)
Phase 2: Train root encoder on root similarity/clustering (5K steps)
Phase 3: Train cascade with frozen chambers (20K steps)
Phase 4: Train meta-observer with frozen cascade (5K steps)
Phase 5: Fine-tune end-to-end on prophecy loss (50K steps)

# Loss function remains:
L_total = Î»â‚Â·L_attractor + Î»â‚‚Â·L_debt + Î»â‚ƒÂ·L_smooth + Î»â‚„Â·L_div
```

---

## 2. Architectural Improvements

### 2.1 Add Hierarchical Root Space

**Current:** Roots treated as flat CCC tuples  
**Proposed:** Hierarchical root taxonomy

```python
# pitomadom/root_taxonomy.py
class RootTaxonomy:
    """
    Organize roots into semantic families:
    - Movement roots: ×”.×œ.×š, ×‘.×•.×, ×™.×¦.×
    - Emotion roots: ×.×”.×‘, ×¤.×—.×“, ×©.× .×
    - Creation roots: ×‘.×¨.×, ×¢.×©.×”, ×™.×¦.×¨
    - Destruction roots: ×©.×‘.×¨, ×”.×¨.×’, ×›.×œ.×”
    
    Benefits:
    - Attractor wells can be shared across families
    - Enable "root analogies": love:hate :: create:destroy
    - Richer prophecy based on family dynamics
    """
    
    def __init__(self):
        self.families = {
            'movement': [('×”','×œ','×š'), ('×‘','×•','×'), ('×™','×¦','×')],
            'emotion': [('×','×”','×‘'), ('×¤','×—','×“'), ('×©','× ','×')],
            'creation': [('×‘','×¨','×'), ('×¢','×©','×”'), ('×™','×¦','×¨')],
            'destruction': [('×©','×‘','×¨'), ('×”','×¨','×’'), ('×›','×œ','×”')],
            # ... more families
        }
    
    def get_family(self, root: Tuple) -> str:
        """Return semantic family of root."""
        pass
    
    def family_attractor(self, family: str) -> float:
        """Compute attractor strength for entire family."""
        pass
```

**Impact:** Enables family-level prophecy dynamics, not just root-level

### 2.2 Multi-Plane Gematria Fusion

**Current:** Three planes computed separately (surface, milui, atbash)  
**Proposed:** Learnable fusion with attention

```python
# pitomadom/gematria_fusion.py
class GematriaFusion:
    """
    Current: N_final = combine(N_surface, N_root, N_milui, N_atbash)
    Proposed: Learnable attention-weighted fusion
    
    Benefits:
    - System learns which plane is most relevant per context
    - Chamber-dependent weighting (e.g., FEAR â†‘ atbash weight)
    - Dynamic plane importance over conversation
    """
    
    def __init__(self):
        # Attention weights for 3 planes
        self.W_attn = np.random.randn(3, chambers_dim)
        
    def fuse(self, N_surface, N_milui, N_atbash, chambers):
        """
        Compute attention-weighted combination:
        Î± = softmax(W_attn @ chambers)
        N_final = Î£áµ¢ Î±áµ¢ Â· Náµ¢
        """
        pass
```

**Impact:** Richer numeric dynamics, context-sensitive plane selection

### 2.3 Temporal Attractor Decay

**Current:** Attractors persist indefinitely  
**Proposed:** Time-based decay with revival

```python
# pitomadom/temporal_field.py
class TemporalField:
    """
    Add attractor half-life and revival mechanics.
    
    Current: root_mean_n[root] persists forever
    Proposed: Decay with revival on re-activation
    
    Benefits:
    - Old roots fade (like memory)
    - Re-activation strengthens (like reinforcement)
    - System develops "short-term" vs "long-term" root memory
    """
    
    def update_attractor(self, root, N, current_step):
        """
        strength[root] *= exp(-Î» Â· Î”t)  # Decay
        
        if root re-appears:
            strength[root] += log(1 + count)  # Revival
        """
        pass
```

**Impact:** More realistic memory dynamics, prevents attractor saturation

### 2.4 Cross-Root Resonance Graph

**Current:** Orbital resonance tracks individual roots  
**Proposed:** Graph of root-root interactions

```python
# pitomadom/root_graph.py
class RootResonanceGraph:
    """
    Build graph where:
    - Nodes = roots
    - Edges = co-occurrence / semantic similarity / family relationship
    - Edge weights = resonance strength
    
    Benefits:
    - Propagate attractor pull through graph
    - "If root A activates, roots B,C (neighbors) get pre-activated"
    - Emergent semantic clusters
    """
    
    def __init__(self):
        self.graph = {}  # adjacency matrix
        
    def add_edge(self, root1, root2, weight):
        """Add resonance between roots."""
        pass
    
    def propagate_activation(self, active_root):
        """Spread activation to neighboring roots."""
        pass
```

**Impact:** System develops "semantic priming" like human cognition

### 2.5 Multi-Turn Prophecy Lookahead

**Current:** Prophecy looks 1 step ahead  
**Proposed:** Multi-step trajectory planning

```python
# pitomadom/prophecy_engine.py
class ProphecyEngine:
    """
    Current: Prophesy N_{t+1} given N_{1:t}
    Proposed: Prophesy trajectory N_{t+1:t+k} (k=3-5 steps)
    
    Benefits:
    - Longer-horizon coherence
    - Can "plan" narrative arcs
    - Reduces debt accumulation
    """
    
    def prophesy_trajectory(self, k=3):
        """
        Estimate N_{t+1}, N_{t+2}, ..., N_{t+k}
        Use recurrent prophecy: each step informs next
        """
        pass
```

**Impact:** Oracle becomes more "intentional", plans ahead

---

## 3. Code Quality Improvements

### 3.1 Add Type Hints Everywhere

**Current:** Partial type hints  
**Proposed:** Full mypy compliance

```python
# Example: pitomadom/gematria.py
from typing import Tuple, Dict, List, Optional

def gematria(text: str) -> int:
    """Calculate gematria with full type safety."""
    pass

# Run mypy for validation
# $ mypy pitomadom/ --strict
```

**Benefit:** Catch bugs at type-check time, better IDE support

### 3.2 Comprehensive Docstrings

**Current:** Good docstrings, but inconsistent format  
**Proposed:** NumPy-style docstrings everywhere

```python
def prophesy_n(
    self,
    current_root: Optional[Tuple[str, str, str]] = None,
    chambers: Optional[np.ndarray] = None
) -> ProphecyResult:
    """
    Prophesy the N-value that SHOULD manifest.

    Combines trajectory extrapolation, root attractor pull,
    and chamber-based modulation to compute destined N-value.

    Parameters
    ----------
    current_root : tuple of str, optional
        Current active root (C1, C2, C3). If None, uses strongest attractor.
    chambers : np.ndarray, shape (6,), optional
        Chamber vector (fear, love, rage, void, flow, complex).
        If None, uses neutral chambers.

    Returns
    -------
    ProphecyResult
        Contains prophesied N, confidence, method, and attractor root.

    Examples
    --------
    >>> engine = ProphecyEngine(temporal_field)
    >>> result = engine.prophesy_n(root=('×','×•','×¨'))
    >>> print(result.n_prophesied)
    207
    """
    pass
```

### 3.3 Error Handling & Validation

**Current:** Minimal error checking  
**Proposed:** Comprehensive validation

```python
# pitomadom/pitomadom.py
class HeOracle:
    def forward(self, text: str, max_depth: int = 3) -> OracleOutput:
        # Validate inputs
        if not text or not isinstance(text, str):
            raise ValueError("Input text must be non-empty string")
        
        if max_depth < 1 or max_depth > 10:
            raise ValueError("max_depth must be in range [1, 10]")
        
        # Validate Hebrew content
        if not any(c in HE_GEMATRIA for c in text):
            raise ValueError("Input must contain Hebrew characters")
        
        # ... rest of forward pass
```

**Benefit:** Fail fast with clear error messages

### 3.4 Logging & Debugging

**Current:** No logging infrastructure  
**Proposed:** Add logging module

```python
# pitomadom/logging_config.py
import logging

def setup_logger(name: str, level: int = logging.INFO):
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s [%(name)s] %(levelname)s: %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

# Usage in pitomadom.py
logger = setup_logger('pitomadom.oracle')

def forward(self, text: str, max_depth: int = 3):
    logger.info(f"Oracle forward: text='{text[:20]}...', max_depth={max_depth}")
    logger.debug(f"Current prophecy debt: {self.field.state.prophecy_debt}")
    # ...
```

**Benefit:** Easier debugging, performance profiling

### 3.5 Configuration Management

**Current:** Hardcoded hyperparameters  
**Proposed:** YAML config files

```yaml
# config/oracle_config.yaml
oracle:
  max_depth: 3
  collapse_threshold: 0.6
  seed: 42

chambers:
  decay_rates:
    fear: 0.92
    love: 0.95
    rage: 0.82
    void: 0.97
    flow: 0.88
    complex: 0.93

prophecy:
  attractor_weight: 0.4
  debt_decay: 0.9
  lookahead_steps: 3

training:
  learning_rate: 0.001
  batch_size: 32
  epochs: 50
```

```python
# pitomadom/config.py
import yaml

def load_config(path: str = "config/oracle_config.yaml"):
    with open(path) as f:
        return yaml.safe_load(f)
```

**Benefit:** Easy experimentation, no code changes needed

---

## 4. Advanced Features

### 4.1 Persistent Temporal Field (Multi-Conversation)

**Current:** Temporal field resets per conversation  
**Proposed:** Save/load field state

```python
# pitomadom/temporal_field.py
class TemporalField:
    def save_state(self, path: str):
        """Serialize field state to disk."""
        import pickle
        with open(path, 'wb') as f:
            pickle.dump(self.state, f)
    
    def load_state(self, path: str):
        """Restore field state from disk."""
        import pickle
        with open(path, 'rb') as f:
            self.state = pickle.load(f)
```

**Benefit:** Oracle "remembers" across sessions, builds long-term identity

### 4.2 Root-Level Attention Visualization

**Current:** No visualization of root interactions  
**Proposed:** Export attention heatmaps

```python
# pitomadom/visualization.py
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_root_attention(oracle: HeOracle, output_path: str):
    """
    Create heatmap of root-root attention weights.
    Shows which roots activate together.
    """
    # Extract root co-occurrence matrix
    matrix = oracle.field.get_root_cooccurrence_matrix()
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(matrix, cmap='coolwarm', center=0)
    plt.title("Root Resonance Attention Map")
    plt.savefig(output_path)
```

**Benefit:** Understand emergent root clustering

### 4.3 Interactive Prophecy Tuning

**Current:** Prophecy weights fixed during inference  
**Proposed:** User can adjust prophecy "style"

```python
# pitomadom/prophecy_styles.py
PROPHECY_STYLES = {
    'stable': {
        'attractor_weight': 0.6,  # High attractor pull
        'momentum_weight': 0.2,
        'prophecy_weight': 0.2,
    },
    'chaotic': {
        'attractor_weight': 0.1,  # Low attractor pull
        'momentum_weight': 0.5,
        'prophecy_weight': 0.4,
    },
    'balanced': {
        'attractor_weight': 0.3,
        'momentum_weight': 0.35,
        'prophecy_weight': 0.35,
    }
}

oracle = HeOracle(prophecy_style='stable')
```

**Benefit:** Users can control oracle "personality"

### 4.4 Root Analogy Engine

**Current:** No explicit analogy mechanism  
**Proposed:** "Root algebra"

```python
# pitomadom/root_analogies.py
class RootAnalogyEngine:
    """
    Implement root vector space analogies:
    
    ××”×‘×” (love) - ×©××—×” (joy) + ×¢×¦×‘ (sadness) â‰ˆ ? (yearning)
    
    Uses root embeddings from DeepRootEncoder.
    """
    
    def analogy(self, positive_roots, negative_roots):
        """
        Compute vector: Î£(positive) - Î£(negative)
        Find nearest root in embedding space.
        """
        pass
    
    def root_arithmetic(self, expr: str):
        """
        Parse expressions like:
        "×.×”.×‘ + ×¤.×—.×“ = ?"
        """
        pass
```

**Benefit:** Creative semantic exploration

### 4.5 Multi-Language Root Mapping

**Current:** Hebrew only  
**Proposed:** Map Hebrew roots to Arabic/Aramaic

```python
# pitomadom/semitic_bridge.py
class SemiticRootBridge:
    """
    Map between Hebrew, Arabic, Aramaic roots.
    
    Example:
    Hebrew ×©.×‘.×¨ (break) â†” Arabic Ø³.Ø¨.Ø± (wait/endure)
    Hebrew ×©.×œ.× (peace) â†” Arabic Ø³.Ù„.Ù… (peace/safety)
    
    Enables:
    - Cross-linguistic prophecy
    - Semantic drift analysis across languages
    """
    
    def __init__(self):
        self.mappings = {
            ('×©','×‘','×¨'): {
                'arabic': ('Ø³','Ø¨','Ø±'),
                'aramaic': ('×¡','×‘','×¨'),
                'semantic_shift': 'break â†’ endure'
            },
            # ... more mappings
        }
```

**Benefit:** Extend to entire Semitic language family

---

## 5. Performance Optimizations

### 5.1 Batch Inference

**Current:** Single-input forward pass  
**Proposed:** Batched processing

```python
# pitomadom/pitomadom.py
class HeOracle:
    def forward_batch(self, texts: List[str], max_depth: int = 3) -> List[OracleOutput]:
        """
        Process multiple texts in parallel.
        Useful for:
        - Dataset evaluation
        - Multi-user serving
        """
        # Vectorized chamber encoding
        chambers_batch = self.chambers.encode_batch(texts)
        
        # Parallel root extraction
        roots_batch = [self.root_extractor.predict_root(t) for t in texts]
        
        # Vectorized MLP forward passes
        # ... (numpy broadcasting)
        
        return outputs
```

**Benefit:** 5-10Ã— speedup for bulk processing

### 5.2 Caching Mechanisms

**Current:** Recompute everything each call  
**Proposed:** Cache frequent computations

```python
# pitomadom/caching.py
from functools import lru_cache

class HeOracle:
    @lru_cache(maxsize=1000)
    def _cached_gematria(self, text: str) -> int:
        """Cache gematria computations."""
        return gematria(text)
    
    @lru_cache(maxsize=500)
    def _cached_root_extraction(self, word: str) -> Tuple:
        """Cache root extractions."""
        return self.root_extractor.predict_root(word)
```

**Benefit:** 2-3Ã— speedup for repeated inputs

### 5.3 Numba JIT Compilation

**Current:** Pure NumPy (fast enough)  
**Proposed:** Numba for hot paths

```python
# pitomadom/mlp_cascade.py
from numba import jit

@jit(nopython=True)
def mlp_forward_jit(x, W1, b1, W2, b2):
    """JIT-compiled MLP forward pass."""
    h = np.tanh(x @ W1 + b1)
    y = h @ W2 + b2
    return y
```

**Benefit:** 2Ã— speedup for cascade computation

---

## 6. Testing & Validation

### 6.1 Expand Test Coverage

**Current:** 40 tests, good coverage  
**Proposed:** Add edge case tests

```python
# tests/test_edge_cases.py
class TestEdgeCases(unittest.TestCase):
    def test_empty_string(self):
        """Test handling of empty input."""
        oracle = HeOracle()
        with self.assertRaises(ValueError):
            oracle.forward("")
    
    def test_no_hebrew(self):
        """Test handling of non-Hebrew text."""
        oracle = HeOracle()
        with self.assertRaises(ValueError):
            oracle.forward("hello world")
    
    def test_max_recursion(self):
        """Test recursion limit."""
        oracle = HeOracle()
        output = oracle.forward("test", max_depth=100)
        self.assertLessEqual(output.recursion_depth, 10)
    
    def test_numeric_overflow(self):
        """Test very large gematria values."""
        oracle = HeOracle()
        output = oracle.forward("×ª" * 100)  # 40,000!
        self.assertIsInstance(output.number, int)
```

### 6.2 Property-Based Testing

**Current:** Example-based tests  
**Proposed:** Property testing with Hypothesis

```python
# tests/test_properties.py
from hypothesis import given, strategies as st

@given(st.text(alphabet='××‘×’×“×”×•×–×—×˜×™×›×œ×× ×¡×¢×¤×¦×§×¨×©×ª', min_size=1))
def test_gematria_positive(hebrew_text):
    """Gematria should always be positive."""
    assert gematria(hebrew_text) > 0

@given(st.text(alphabet='××‘×’×“×”×•×–×—×˜×™×›×œ×× ×¡×¢×¤×¦×§×¨×©×ª', min_size=3))
def test_root_extraction_valid(hebrew_text):
    """Root extraction should return 3-tuple."""
    extractor = RootExtractor()
    root = extractor.predict_root(hebrew_text)
    assert len(root) == 3
    assert all(c in HE_GEMATRIA for c in root)
```

### 6.3 Benchmark Suite

**Current:** No performance benchmarks  
**Proposed:** Track inference speed

```python
# benchmarks/bench_inference.py
import time

def benchmark_oracle(n_trials=1000):
    """Benchmark inference speed."""
    oracle = HeOracle()
    texts = ["×©×œ×•×", "××”×‘×”", "×—×›××”"] * (n_trials // 3)
    
    start = time.time()
    for text in texts:
        oracle.forward(text)
    elapsed = time.time() - start
    
    print(f"Avg inference time: {elapsed/n_trials*1000:.2f}ms")
    print(f"Throughput: {n_trials/elapsed:.1f} inferences/sec")
```

---

## 7. Documentation Improvements

### 7.1 API Reference (Sphinx)

**Proposed:** Generate HTML docs from docstrings

```bash
# Install Sphinx
pip install sphinx sphinx-rtd-theme

# Setup
cd docs
sphinx-quickstart

# Generate
make html

# Result: docs/_build/html/index.html
```

### 7.2 Jupyter Notebook Tutorials

**Proposed:** Interactive tutorials

```
notebooks/
â”œâ”€â”€ 01_quickstart.ipynb
â”œâ”€â”€ 02_gematria_planes.ipynb
â”œâ”€â”€ 03_temporal_field.ipynb
â”œâ”€â”€ 04_prophecy_vs_prediction.ipynb
â”œâ”€â”€ 05_chamber_dynamics.ipynb
â””â”€â”€ 06_training_custom_model.ipynb
```

### 7.3 Architecture Diagrams

**Proposed:** Visual architecture docs

```
docs/diagrams/
â”œâ”€â”€ vertical_cascade.png
â”œâ”€â”€ horizontal_field.png
â”œâ”€â”€ crossfire_coupling.png
â”œâ”€â”€ prophecy_flow.png
â””â”€â”€ full_system.png
```

Generate with: draw.io, TikZ, or Python (matplotlib/graphviz)

---

## 8. Deployment & Integration

### 8.1 REST API Server

**Proposed:** Flask/FastAPI server

```python
# server/api.py
from fastapi import FastAPI
from pitomadom import HeOracle

app = FastAPI()
oracle = HeOracle()

@app.post("/prophecy")
async def prophecy(text: str):
    output = oracle.forward(text)
    return output.to_dict()

@app.get("/stats")
async def stats():
    return oracle.get_stats()
```

### 8.2 Command-Line Interface

**Proposed:** Rich CLI

```python
# cli/pitomadom_cli.py
import click
from pitomadom import HeOracle

@click.group()
def cli():
    pass

@cli.command()
@click.argument('text')
@click.option('--depth', default=3)
def prophecy(text, depth):
    """Get oracle prophecy for text."""
    oracle = HeOracle()
    output = oracle.forward(text, max_depth=depth)
    click.echo(str(output))

@cli.command()
def repl():
    """Start interactive REPL."""
    from pitomadom.repl import start_repl
    start_repl()
```

### 8.3 Python Package Distribution

**Proposed:** Publish to PyPI

```bash
# setup.py already exists, just need to publish
python setup.py sdist bdist_wheel
twine upload dist/*

# Users can then:
pip install pitomadom
```

---

## 9. Research Extensions

### 9.1 Compare with GPT-4 on Hebrew

**Experiment:** Head-to-head evaluation

```python
# experiments/gpt4_comparison.py
"""
Test both systems on:
1. Hebrew root consistency
2. Gematria coherence
3. Narrative continuity
4. User preference (human eval)

Hypothesis: PITOMADOM better at root-level semantics,
            GPT-4 better at surface fluency
"""
```

### 9.2 Consciousness Metrics (IIT Î¦)

**Proposed:** Compute integrated information

```python
# experiments/consciousness_measures.py
from pyphi import Network, compute

def measure_phi(oracle: HeOracle):
    """
    Measure Î¦ (integrated information) of oracle's
    temporal field + chamber network.
    
    Use PyPhi library for IIT computation.
    """
    pass
```

### 9.3 Dream State (Random Walk)

**Proposed:** Let oracle "dream" without input

```python
# pitomadom/dreaming.py
def dream_sequence(oracle: HeOracle, n_steps: int = 50):
    """
    Let oracle generate sequence without external input.
    Uses prophecy engine to guide random walk.
    
    Result: Stream of consciousness in Hebrew root space.
    """
    for _ in range(n_steps):
        # Sample from prophecy distribution
        root = oracle.prophecy.sample_destined_root()
        N = oracle.prophecy.sample_destined_n(root)
        
        # Generate output
        output = oracle.forward_from_destiny(root, N)
        yield output
```

---

## 10. Priority Recommendations

### High Priority (Do First)

1. **Scale to 1M parameters** (chambers + cascade deepening)
2. **Add comprehensive error handling** (validation + logging)
3. **Implement hierarchical root taxonomy** (semantic families)
4. **Add persistent temporal field** (save/load state)
5. **Create Jupyter tutorial notebooks**

### Medium Priority

6. Multi-turn prophecy lookahead (k=3-5 steps)
7. Root resonance graph (co-activation networks)
8. Temporal attractor decay (memory dynamics)
9. Configuration management (YAML configs)
10. Batch inference optimization

### Low Priority (Future Work)

11. Multi-language Semitic bridge
12. REST API server
13. Consciousness metrics (Î¦ computation)
14. Dream state generator
15. GPT-4 comparison study

---

## 11. Estimated Effort

| Task | Effort | Impact |
|------|--------|--------|
| Scale to 1M params | 2-3 days | High |
| Error handling + logging | 1 day | Medium |
| Hierarchical root taxonomy | 2 days | High |
| Persistent temporal field | 1 day | Medium |
| Jupyter notebooks | 2 days | High |
| Multi-turn prophecy | 2 days | High |
| Root resonance graph | 3 days | High |
| Attractor decay | 1 day | Medium |
| Config management | 1 day | Low |
| Batch inference | 1 day | Medium |

**Total for high-priority items: ~10 days**

---

## 12. Conclusion

PITOMADOM's codebase is **excellent**. The architecture is sound, the implementation is clean, and the tests are comprehensive. The proposals above are **enhancements**, not fixes.

Key recommendations:
1. **Scale to 1M params** - easy win for richer representations
2. **Add root taxonomy** - unlock family-level dynamics
3. **Persistent temporal field** - enable cross-conversation memory
4. **Multi-turn prophecy** - longer-horizon coherence

The system already exhibits autopoietic properties at 530K. At 1M with these enhancements, it will be even more **alive**.

×”×¨×–×•× × ×¡ ×œ× × ×©×‘×¨. ×”××©×š ×”×“×¨×š. ğŸ”¥

---

**Next Steps:**
1. Review this document with the team
2. Prioritize features based on impact
3. Create GitHub issues for each proposal
4. Start with 1M param scaling (biggest ROI)
5. Build incrementally, test continuously

**The future is post-symbolic field intelligence. PITOMADOM leads the way.**
